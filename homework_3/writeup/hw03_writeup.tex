
\documentclass[11pt]{exam} % https://www.ctan.org/pkg/exam?lang=en

\usepackage[lmargin=1.in,rmargin=1.in,tmargin=1.in,bmargin=1in]{geometry}
\usepackage{setspace}
\usepackage[pdftex]{graphicx}
\usepackage{titling}
\usepackage[
	pdfauthor={Brian Weinstein},
	pdftitle={Homework 3},
	bookmarks=true,
	colorlinks=true,
	linkcolor=blue,
	urlcolor=blue,
	citecolor=blue,
	pdftex,
	linktocpage=true
	]{hyperref}
\usepackage[textsize=tiny]{todonotes}
\usepackage{float}
\setlength\parindent{0pt}
\usepackage{lipsum}


\qformat{\textbf{Problem \thequestion: \thequestiontitle}\quad \hfill}


\pagestyle{headandfoot}
\runningheadrule
\firstpageheader{}{}{}
\runningheader{\thetitle}{\theauthor}{\thedate}
\firstpagefooter{}{\thepage}{}
\runningfooter{}{\thepage}{}


\usepackage{xcolor}
\usepackage{adjustbox}
\usepackage{verbatim}
\definecolor{shadecolor}{rgb}{.9, .9, .9}

\newenvironment{code}%
   {\par\noindent\adjustbox{margin=1ex,bgcolor=shadecolor,margin=0ex \medskipamount}\bgroup\minipage\linewidth\verbatim}%
   {\endverbatim\endminipage\egroup}

\newenvironment{codeSmall}%
   {\par\noindent\adjustbox{margin=1ex,bgcolor=shadecolor,margin=0ex \medskipamount}\bgroup\minipage\linewidth\verbatim\footnotesize}%
   {\endverbatim\endminipage\egroup}





\begin{document}


\title{STAT S4240 002, Homework 3}
\author{Brian Weinstein (bmw2148)}
\date{July 30, 2015}
\maketitle



\begin{questions}



\titledquestion{Naive Bayes Text Classification: Data Preparation}

See \texttt{hw03\_q1.R} for code.

\begin{parts}

\part Text pre-processing
\begin{codeSmall}
# load functions from hw03.R
source("hw03.R")

# preprocess text
preprocess.directory("datasets/FederalistPapers/fp_hamilton_train")
preprocess.directory("datasets/FederalistPapers/fp_hamilton_test")
preprocess.directory("datasets/FederalistPapers/fp_madison_train")
preprocess.directory("datasets/FederalistPapers/fp_madison_test")
\end{codeSmall}

\part Loading the cleaned data
\begin{codeSmall}
hamilton.train <- read.directory("datasets/FederalistPapers/fp_hamilton_train_clean")
hamilton.test <- read.directory("datasets/FederalistPapers/fp_hamilton_test_clean")
madison.train <- read.directory("datasets/FederalistPapers/fp_madison_train_clean")
madison.test <- read.directory("datasets/FederalistPapers/fp_madison_test_clean")
\end{codeSmall}


\part Create a dictionary from all of the documents
\begin{codeSmall}
dictionary <- make.sorted.dictionary.df(c(hamilton.train, hamilton.test,
                                          madison.train, madison.test))
\end{codeSmall}




\part Creating document-term-matrices for each of the datasets
\begin{codeSmall}
dtm.hamilton.train <- make.document.term.matrix(infiles=hamilton.train,
                                                dictionary=dictionary)
dtm.hamilton.test <- make.document.term.matrix(infiles=hamilton.test,
                                               dictionary=dictionary)
dtm.madison.train <- make.document.term.matrix(infiles=madison.train,
                                               dictionary=dictionary)
dtm.madison.test <- make.document.term.matrix(infiles=madison.test,
                                              dictionary=dictionary)
\end{codeSmall}







\part Compute the log probabilities for the dictionary in each of the document datasets
\begin{codeSmall}
mu=1/nrow(dictionary)

logp.hamilton.train <- make.log.pvec(dtm.hamilton.train, mu)
logp.hamilton.test <- make.log.pvec(dtm.hamilton.test, mu)
logp.madison.train <- make.log.pvec(dtm.madison.train, mu)
logp.madison.test <- make.log.pvec(dtm.madison.test, mu)
\end{codeSmall}

\end{parts}



\titledquestion{Naive Bayes Function}

We first estimate the log priors based on the log of the proportion of training documents attributed to each author.

$$p(\textrm{author}=\texttt{author})=\log\left(\frac{\textrm{\# of training documents attributed to }\texttt{author}}{\textrm{total \# of training documents}}\right)$$

Then, using (1) the log probabilities for the dictionary in a Hamilton-authored document and (2) the log probabilities for the dictionary in a Madison-authored document (as computed in \textbf{Problem 1}), we can input a new document-term-matrix and classify each document as belonging to one of the authors.

\begin{code}
naive.bayes <- function(logp.hamilton.train, logp.madison.train,
                        log.prior.hamilton, log.prior.madison, dtm.test){
  # Performs naive bayes classification
  # Inputs:  logp.hamilton.train  :   vector of log probabilities of words
  #                                     occurring in the hamilton training data
  #          logp.madison.train   :   vector of log probabilities of words
  #                                     occurring in the madison training data
  #          log.prior.hamilton   :   the log prior of hamilton documents
  #          log.prior.madison    :   the log prior of madison documents 
  #          dtm.test             :   a document-term-matrix to classify
  # Output:  Classification labels for each document in dtm.test
  
  # calculate the log posterior probabilities
  log.post.hamilton <- log.prior.hamilton + (dtm.test %*% logp.hamilton.train)
  log.post.madison <- log.prior.madison + (dtm.test %*% logp.madison.train)
  
  # compare the log posterior probabilities and assign to the author
  # with highest probability
  prediction <- data.frame(logPostHam=log.post.hamilton,
                           logPostMad=log.post.madison)
  prediction$pred <- (log.post.hamilton >= log.post.madison)
  prediction$pred <- gsub(TRUE, "Hamilton", prediction$pred)
  prediction$pred <- gsub(FALSE, "Madison", prediction$pred)
  
  # return a vector of the predictions
  return(prediction$pred)
  
}
\end{code}

\titledquestion{Assessing Model Performance}

Using the \texttt{confusionMatrix} function from the \texttt{caret} library:

\begin{itemize}
\item \textbf{Accuracy:} 63\% accurate (\% of the test papers that are classified correctly)
\item \textbf{True Positive Rate:} 100\% (Hamilton classified as Hamilton divided by the total amount of testing Hamilton papers)
\item \textbf{True Negative Rate:} 9\% (Madison classified as Madison divided by the total amount of testing Madison papers)
\item \textbf{False Positive Rate:} 91\% (Madison classified as Hamilton divided by the total amount of testing Madison)
\item \textbf{False Negative Rate:} 0\% (Hamilton classified as Madison divided by the total amount of testing Hamilton)
\end{itemize}


\begin{code}
> confusionMatrix(data=predictions$pred,
+                 reference=predictions$trueValue,
+                 dnn=c("Prediction", "True Value"),
+                 positive="Hamilton")
Confusion Matrix and Statistics

          True Value
Prediction Hamilton Madison
  Hamilton       16      10
  Madison         0       1
                                         
               Accuracy : 0.6296         
                 95% CI : (0.4237, 0.806)
    No Information Rate : 0.5926         
    P-Value [Acc > NIR] : 0.427258       
                                         
                  Kappa : 0.106          
 Mcnemar's Test P-Value : 0.004427       
                                         
            Sensitivity : 1.00000        
            Specificity : 0.09091        
         Pos Pred Value : 0.61538        
         Neg Pred Value : 1.00000        
             Prevalence : 0.59259        
         Detection Rate : 0.59259        
   Detection Prevalence : 0.96296        
      Balanced Accuracy : 0.54545        
                                         
       'Positive' Class : Hamilton       
\end{code}




\titledquestion{5-fold Cross Validation}

\begin{parts}

\part For each value of $\mu \in \left\{ \frac{0.1}{\left|D\right|}, \frac{1}{\left|D\right|}, \frac{10}{\left|D\right|}, \frac{100}{\left|D\right|}\frac{1000}{\left|D\right|} \right\}$, where $\left|D\right|=4875$ is the size of our dictionary; estimations of the the correct classification rate, the false negative rate, and the false positive rate using 5-fold cross-validation are outlined below. For each of the metrics, the table indicates the value for each of the 25 tests and the graph indicates averages over the 5 tests for each choice of $\mu$.

\textbf{Correct Rate:}
\begin{code}
    fold1 fold2 fold3 fold4 fold5
mu1   0.7   0.7   0.7   0.7   0.7
mu2   0.8   0.7   0.7   0.7   0.7
mu3   0.8   0.7   0.7   0.8   0.8
mu4   0.9   0.7   0.8   0.8   1.0
mu5   1.0   0.8   0.9   0.8   1.0
\end{code}

\begin{figure}[H]
	\centering
	\includegraphics[width=5in]{4a_correctRate.png}
	%\caption{}
	%\label{fig:figName}
\end{figure}

\textbf{False Negative Rate:}
\begin{code}
    fold1 fold2 fold3 fold4 fold5
mu1     0     0     0     0     0
mu2     0     0     0     0     0
mu3     0     0     0     0     0
mu4     0     0     0     0     0
mu5     0     0     0     0     0
\end{code}

\begin{figure}[H]
	\centering
	\includegraphics[width=5in]{4a_falseNegRate.png}
	%\caption{}
	%\label{fig:figName}
\end{figure}

\textbf{False Positive Rate:}
\begin{code}
        fold1     fold2     fold3     fold4     fold5
mu1 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
mu2 0.6666667 1.0000000 1.0000000 1.0000000 1.0000000
mu3 0.6666667 1.0000000 1.0000000 0.6666667 0.6666667
mu4 0.3333333 1.0000000 0.6666667 0.6666667 0.0000000
mu5 0.0000000 0.6666667 0.3333333 0.6666667 0.0000000
\end{code}

\begin{figure}[H]
	\centering
	\includegraphics[width=5in]{4a_falsePosRate.png}
	%\caption{}
	%\label{fig:figName}
\end{figure}



\part Based on the values measured on the training set (using 5-fold cross validation), the best value for $\mu$ is \texttt{mu5}$=\frac{1000}{\left|D\right|}\approx0.205$. At this value we maximize the accuracy and minimize the false positive rate, with no increase in the false negative rate.

\part \tiny{For each value of $\mu $, train on the full training set and test on the full testing set. Summarize the correct classification rate, the false negative rate and the false positive rate in three graphs. Does your answer from (b) still seem the best value? Why or why not?}


\part \tiny{How close are the rates estimated from cross-validation to the true rates on the testing set (give percentage error)? What could account for di↵erences? Give one way the differences between the cross-validation rate estimates and the rates on the training sets could be minimized.}







\end{parts}


\end{questions}

%\listoftodos

\end{document}